{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "segmenattion.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNZFMBVBSBzW/xbQBdIg5Ii",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlenaAntipina/PytorchLearning/blob/main/segmenattion_learn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib as plt\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "\n",
        "import torch \n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n"
      ],
      "metadata": {
        "id": "4SBOLl00Eozx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torchmetrics"
      ],
      "metadata": {
        "id": "bn2G9qW5FX7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchmetrics"
      ],
      "metadata": {
        "id": "g4OohDCsFz-z"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pytorch-lightning lightning-bolts\n"
      ],
      "metadata": {
        "id": "Dlu4t8OEFzET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n"
      ],
      "metadata": {
        "id": "Z2vXItWCE9C0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import albumentations as A"
      ],
      "metadata": {
        "id": "WWKYfo6ZGBwq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_HEIGHT = 128\n",
        "IMG_WIDTH = 128\n",
        "BATCH_SIZE = 100\n",
        "EPOCHS = 25\n",
        "SAMPLES = 7000"
      ],
      "metadata": {
        "id": "Ol5NFtNPGHnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root = Path(\"../input/aisegmentcom-matting-human-datasets/\")"
      ],
      "metadata": {
        "id": "5_G5OnmQGK8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cutout_paths= sorted(list(root.glob(\"matting/*/*/*\")))\n",
        "image_paths = sorted(list(root.glob(\"clip_img/*/*/*\")))"
      ],
      "metadata": {
        "id": "Zk8gXZtYGK2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "im_pths = image_paths[:2500]\n",
        "cut_pths = cutout_paths[:2500]"
      ],
      "metadata": {
        "id": "ImVgkIWEGSnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = np.frompyfunc(lambda x, y: x.stem != y.stem, 2, 1)\n",
        "print(f\"Total # of mismatches: {f(im_pths, cut_pths).sum()}\")"
      ],
      "metadata": {
        "id": "BJb-QRg2GXIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(1, 2)\n",
        "im = cv2.imread(im_pths[0].as_posix(), cv2.IMREAD_UNCHANGED)\n",
        "img = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
        "cut = cv2.imread(cut_pths[0].as_posix(), cv2.IMREAD_UNCHANGED)\n",
        "cutg = cv2.cvtColor(cut, cv2.COLOR_BGR2RGB)\n",
        "axs[0].imshow(img)\n",
        "axs[0].set_title(\"Image\")\n",
        "axs[0].axis(\"off\")\n",
        "axs[1].imshow(cutg)\n",
        "axs[1].set_title(\"Cutout\")\n",
        "axs[1].axis(\"off\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1A9XS1ZCGZgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({\n",
        "    \"image_paths\": im_pths,\n",
        "    \"cutout_paths\": cut_pths\n",
        "})\n",
        "df.head()"
      ],
      "metadata": {
        "id": "4PIeduTbGcFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SegDataset(Dataset):\n",
        "    def __init__(self, df, transforms=None):\n",
        "        self.image_paths = df.image_paths\n",
        "        self.cutout_paths = df.cutout_paths\n",
        "        self.transfroms = transforms\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        im = cv2.imread(self.image_paths[idx].as_posix(), cv2.IMREAD_UNCHANGED)\n",
        "        img = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
        "        cut = cv2.imread(cut_pths[idx].as_posix(), cv2.IMREAD_UNCHANGED)\n",
        "        mask = np.where(cut[:, :, 3] > 0, 1, 0)\n",
        "        \n",
        "        if self.transfroms is not None:\n",
        "            transformed = transform(image=im, mask=mask)\n",
        "            transformed_image = transformed['image']\n",
        "            transformed_mask = transformed['mask']\n",
        "\n",
        "            # make channels first\n",
        "            transformed_image = np.transpose(transformed_image, (2, 1, 0))\n",
        "            transformed_mask = np.expand_dims(transformed_mask, 0)\n",
        "            \n",
        "            return {\n",
        "                \"image\": torch.tensor(transformed_image, dtype=torch.float32), \n",
        "                \"mask\": torch.tensor(transformed_mask, dtype=torch.float32) \n",
        "            }\n",
        "        \n",
        "        else:\n",
        "            # make channels first\n",
        "            img = np.transpose(img, (2, 1, 0))\n",
        "            mask = np.expand_dims(mask, 0)\n",
        "        \n",
        "            return {\n",
        "                \"image\": torch.tensor(img, dtype=torch.float32),\n",
        "                \"mask\": torch.tensor(mask, dtype=torch.float32)\n",
        "            }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def diplay_sample(self, idx):\n",
        "        im = cv2.imread(self.image_paths[idx].as_posix(), cv2.IMREAD_UNCHANGED)\n",
        "        img = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
        "        cut = cv2.imread(cut_pths[idx].as_posix(), cv2.IMREAD_UNCHANGED)\n",
        "        cutg = cv2.cvtColor(cut, cv2.COLOR_BGR2RGB)\n",
        "        mask = cut[:, :, 3]\n",
        "        \n",
        "        fig, axs = plt.subplots(1, 3)\n",
        "        \n",
        "        axs[0].imshow(img)\n",
        "        axs[0].set_title(\"Image\")\n",
        "        axs[0].axis(\"off\")\n",
        "        axs[1].imshow(cutg)\n",
        "        axs[1].set_title(\"Cutout\")\n",
        "        axs[1].axis(\"off\")\n",
        "        axs[2].imshow(mask)\n",
        "        axs[2].set_title(\"Mask\")\n",
        "        axs[2].axis(\"off\")\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "FD6JrDwNGe9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = A.Compose([\n",
        "    A.Resize(width=IMG_WIDTH, height=IMG_HEIGHT),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.Normalize(),\n",
        "    A.RandomBrightnessContrast(p=0.2),\n",
        "])"
      ],
      "metadata": {
        "id": "xOHw2n21GwpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = SegDataset(df=df, transforms=transform)"
      ],
      "metadata": {
        "id": "D3SRYBAEG0dP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds[0][\"image\"].shape, ds[0][\"mask\"].shape"
      ],
      "metadata": {
        "id": "WnMYDwoYG2ac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(ds[0][\"mask\"])"
      ],
      "metadata": {
        "id": "w1CDzTH2G3-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds.diplay_sample(0)"
      ],
      "metadata": {
        "id": "x7R832pMG50_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DoubleConvSame(nn.Module):\n",
        "    def __init__(self, c_in, c_out):\n",
        "        super(DoubleConvSame, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=c_in, out_channels=c_out, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=c_out, out_channels=c_out, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "    \n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.conv = DoubleConvSame(c_in=in_channels, c_out=in_channels * 2)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        c = self.conv(x)\n",
        "        p = self.pool(c)\n",
        "\n",
        "        return c, p"
      ],
      "metadata": {
        "id": "k8Js6LO9G7f3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionDecoder(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(AttentionDecoder, self).__init__()\n",
        "\n",
        "        self.up_conv = DoubleConvSame(c_in=in_channels, c_out=in_channels // 2)\n",
        "        self.up = nn.ConvTranspose2d(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=in_channels // 2,\n",
        "            kernel_size=2,\n",
        "            stride=2,\n",
        "        )\n",
        "\n",
        "    def forward(self, conv1, conv2, attn):\n",
        "        up = self.up(conv1)\n",
        "        mult = torch.multiply(attn, up)\n",
        "        cat = torch.cat([mult, conv2], dim=1)\n",
        "        uc = self.up_conv(cat)\n",
        "\n",
        "        return uc\n",
        "\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, g_chl, x_chl):\n",
        "        super(AttentionBlock, self).__init__()\n",
        "\n",
        "        inter_shape = x_chl // 4\n",
        "\n",
        "        # Conv 1x1 with stride 2 for `x`\n",
        "        self.conv_x = nn.Conv2d(\n",
        "            in_channels=x_chl,\n",
        "            out_channels=inter_shape,\n",
        "            kernel_size=1,\n",
        "            stride=2,\n",
        "        )\n",
        "\n",
        "        # Conv 1x1 with stride 1 for `g` (gating signal)\n",
        "        self.conv_g = nn.Conv2d(\n",
        "            in_channels=g_chl,\n",
        "            out_channels=inter_shape,\n",
        "            kernel_size=1,\n",
        "            stride=1,\n",
        "        )\n",
        "\n",
        "        # Conv 1x1 for `psi` the output after `g` + `x`\n",
        "        self.psi = nn.Conv2d(\n",
        "            in_channels=inter_shape,\n",
        "            out_channels=1,\n",
        "            kernel_size=1,\n",
        "            stride=1,\n",
        "        )\n",
        "\n",
        "        # For upsampling the attention output to size of `x`\n",
        "        self.upsample = nn.Upsample(scale_factor=2)\n",
        "\n",
        "    def forward(self, g, x):\n",
        "\n",
        "        # perform the convs on `x` and `g`\n",
        "        theta_x = self.conv_x(x)\n",
        "        gate = self.conv_g(g)\n",
        "\n",
        "        # `theta_x` + `gate`\n",
        "        add = theta_x + gate\n",
        "\n",
        "        # ReLU on the add operation\n",
        "        relu = torch.relu(add)\n",
        "\n",
        "        # the 1x1 Conv\n",
        "        psi = self.psi(relu)\n",
        "\n",
        "        # Sigmoid to squash the outputs/attention weights\n",
        "        sig = torch.sigmoid(psi)\n",
        "\n",
        "        # Upsample to original size of `x` to perform multiplication\n",
        "        upsample = self.upsample(sig)\n",
        "\n",
        "        # return the attention weights!\n",
        "        return upsample\n",
        "\n",
        "\n",
        "class AttentionUNet(nn.Module):\n",
        "    def __init__(self, c_in, c_out):\n",
        "        super(AttentionUNet, self).__init__()\n",
        "\n",
        "        self.conv1 = DoubleConvSame(c_in=c_in, c_out=64)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.enc1 = Encoder(64)\n",
        "        self.enc2 = Encoder(128)\n",
        "        self.enc3 = Encoder(256)\n",
        "        self.enc4 = Encoder(512)\n",
        "\n",
        "        self.conv5 = DoubleConvSame(c_in=512, c_out=1024)\n",
        "\n",
        "        self.attn1 = AttentionBlock(1024, 512)\n",
        "        self.attn2 = AttentionBlock(512, 256)\n",
        "        self.attn3 = AttentionBlock(256, 128)\n",
        "        self.attn4 = AttentionBlock(128, 64)\n",
        "\n",
        "        self.attndeco1 = AttentionDecoder(1024)\n",
        "        self.attndeco2 = AttentionDecoder(512)\n",
        "        self.attndeco3 = AttentionDecoder(256)\n",
        "        self.attndeco4 = AttentionDecoder(128)\n",
        "\n",
        "        self.conv_1x1 = nn.Conv2d(in_channels=64, out_channels=c_out, kernel_size=1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"ENCODER\"\"\"\n",
        "\n",
        "        c1 = self.conv1(x)\n",
        "        p1 = self.pool(c1)\n",
        "\n",
        "        c2, p2 = self.enc1(p1)\n",
        "        c3, p3 = self.enc2(p2)\n",
        "        c4, p4 = self.enc3(p3)\n",
        "\n",
        "        \"\"\"BOTTLE-NECK\"\"\"\n",
        "\n",
        "        c5 = self.conv5(p4)\n",
        "\n",
        "        \"\"\"DECODER - WITH ATTENTION\"\"\"\n",
        "\n",
        "        att1 = self.attn1(c5, c4)\n",
        "        uc1 = self.attndeco1(c5, c4, att1)\n",
        "\n",
        "        att2 = self.attn2(uc1, c3)\n",
        "        uc2 = self.attndeco2(c4, c3, att2)\n",
        "\n",
        "        att3 = self.attn3(uc2, c2)\n",
        "        uc3 = self.attndeco3(c3, c2, att3)\n",
        "\n",
        "        att4 = self.attn4(uc3, c1)\n",
        "        uc4 = self.attndeco4(c2, c1, att4)\n",
        "\n",
        "        outputs = self.conv_1x1(uc4)\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "xROljJw0HBTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attn_unet = AttentionUNet(3, 1)"
      ],
      "metadata": {
        "id": "x-8zDpsQHVe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attn_unet"
      ],
      "metadata": {
        "id": "z2x_u9kXHW0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SegDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, df):\n",
        "        super().__init__()\n",
        "        self.dataset = SegDataset(df, transforms=transform)\n",
        "\n",
        "    def setup(self, stage) -> None:\n",
        "        if stage == \"fit\" or stage is None:\n",
        "            lengths = [int(len(self.dataset) * 0.8), int(len(self.dataset) * 0.2)]\n",
        "            self.train_data, self.val_data = random_split(self.dataset, lengths)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_data, batch_size=BATCH_SIZE, num_workers=2)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val_data, batch_size=BATCH_SIZE, num_workers=2)"
      ],
      "metadata": {
        "id": "Bub8Ezr7HaHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LitModel(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super(LitModel, self).__init__()\n",
        "        self.model = AttentionUNet(3, 1)\n",
        "        self.loss = nn.BCEWithLogitsLoss()\n",
        "        self.train_acc = torchmetrics.Accuracy()\n",
        "        self.val_acc = torchmetrics.Accuracy()\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return optim.Adam(self.model.parameters())\n",
        "\n",
        "    def forward(self, images):\n",
        "        return self.model(images)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        images = batch[\"image\"]\n",
        "        masks = batch[\"mask\"]\n",
        "        \n",
        "        preds = self.forward(images)\n",
        "        loss = self.loss(input=preds, target=masks)\n",
        "        acc = self.train_acc(preds, masks.int())\n",
        "\n",
        "        self.log(\"train_loss\", loss)\n",
        "        self.log(\"train_acc\", acc, prog_bar=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        images = batch[\"image\"]\n",
        "        masks = batch[\"mask\"]\n",
        "        \n",
        "        preds = self.forward(images)\n",
        "        loss = self.loss(input=preds, target=masks)\n",
        "        acc = self.val_acc(preds, masks.int())\n",
        "\n",
        "        self.log(\"val_loss\", loss)\n",
        "        self.log(\"val_acc\", acc, prog_bar=True)\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "ENf7Qwk2HckC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LitModel()\n",
        "dm = SegDataModule(df=df)\n",
        "\n",
        "# for checkpointing our model\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    dirpath=\"../working/models\", \n",
        "    monitor=\"val_acc\", \n",
        "    mode=\"max\", \n",
        "    verbose=True,\n",
        "    save_top_k=3,\n",
        "    filename='{epoch}-{val_loss:.2f}-{val_acc:.2f}'\n",
        ")\n",
        "\n",
        "early_stop_callback = EarlyStopping(\n",
        "    monitor=\"val_acc\", \n",
        "    min_delta=0.00, \n",
        "    patience=3, \n",
        "    verbose=True, \n",
        "    mode=\"max\"\n",
        ")\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    logger=True,\n",
        "    max_epochs=EPOCHS,\n",
        "    accelerator=\"gpu\", \n",
        "    callbacks=[checkpoint_callback, early_stop_callback],\n",
        ")\n",
        "\n",
        "trainer.fit(model, datamodule=dm)"
      ],
      "metadata": {
        "id": "bfHKiJkBHjC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(image_path):\n",
        "    # Read the image, copy, resize the copy\n",
        "    im = cv2.imread(image_path.as_posix(), cv2.IMREAD_UNCHANGED)\n",
        "    test_image = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
        "    test_image_copy = cv2.resize(test_image, (IMG_HEIGHT, IMG_WIDTH))\n",
        "    \n",
        "    # resize, transpose and create batch dimension\n",
        "    test_image = cv2.resize(test_image, (IMG_HEIGHT, IMG_WIDTH))\n",
        "    test_image = np.transpose(test_image, (2, 1, 0))\n",
        "    test_image = torch.unsqueeze(torch.tensor(test_image), 0)\n",
        "    \n",
        "    # Load the model\n",
        "    best_model = LitModel.load_from_checkpoint(checkpoint_callback.best_model_path)\n",
        "    \n",
        "    # Make the prediction\n",
        "    pred = best_model(test_image.float())\n",
        "    pred = pred.detach().numpy()[0]\n",
        "    pred = np.transpose(pred, (2, 1, 0))\n",
        "    preds_test_thresh = (pred >= 0.5).astype(np.uint8)\n",
        "    alpha_preds = preds_test_thresh * 255\n",
        "    predicted_mask = np.concatenate((test_image_copy, alpha_preds), axis=-1)\n",
        "\n",
        "    fig, axs = plt.subplots(1, 2)\n",
        "\n",
        "    axs[0].imshow(test_image_copy)\n",
        "    axs[0].set_title(\"Image\")\n",
        "    axs[0].axis(\"off\")\n",
        "    axs[1].imshow(predicted_mask)\n",
        "    axs[1].set_title(\"Prediction\")\n",
        "    axs[1].axis(\"off\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "1kZSZ5TGHnCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(10):\n",
        "    predict(image_paths[np.random.randint(5001, len(image_paths))])"
      ],
      "metadata": {
        "id": "KvfO1UQEHrcF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}