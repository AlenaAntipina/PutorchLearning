{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPz00s/cd3tKLnmipgcopvu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlenaAntipina/PytorchLearning/blob/firstTest/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PAQ5pFWdXwnm"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TinyModel(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(TinyModel, self).__init__()\n",
        "\n",
        "    self.linear1 = torch.nn.Linear(100, 200)\n",
        "    self.activation = torch.nn.ReLU()\n",
        "    self.linear2 = torch.nn.Linear(200, 10)\n",
        "    self.sooftmax = torch.nn.Softmax()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.linear1(x)\n",
        "    x = self.activation(x)\n",
        "    x = self.linear2(x)\n",
        "    x = self.softmax(x)\n",
        "    return x\n",
        "\n",
        "tinymodel = TinyModel()\n",
        "\n",
        "print('The model:')\n",
        "print(tinymodel)\n",
        "\n",
        "print('\\n\\nJust one layer:')\n",
        "print(tinymodel.linear2)\n",
        "\n",
        "print('\\n\\nModel params:')\n",
        "for param in tinymodel.parameters():\n",
        "    print(param)\n",
        "\n",
        "print('\\n\\nLayer params:')\n",
        "for param in tinymodel.linear2.parameters():\n",
        "    print(param)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYCV1Udcb5Lq",
        "outputId": "a9f9b080-cd4f-4920-edd1-21ed1b2cb5b7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model:\n",
            "TinyModel(\n",
            "  (linear1): Linear(in_features=100, out_features=200, bias=True)\n",
            "  (activation): ReLU()\n",
            "  (linear2): Linear(in_features=200, out_features=10, bias=True)\n",
            "  (sooftmax): Softmax(dim=None)\n",
            ")\n",
            "\n",
            "\n",
            "Just one layer:\n",
            "Linear(in_features=200, out_features=10, bias=True)\n",
            "\n",
            "\n",
            "Model params:\n",
            "Parameter containing:\n",
            "tensor([[ 0.0872, -0.0740, -0.0341,  ..., -0.0238,  0.0160,  0.0708],\n",
            "        [ 0.0230, -0.0832,  0.0353,  ...,  0.0874, -0.0532, -0.0519],\n",
            "        [ 0.0766, -0.0643,  0.0102,  ..., -0.0167, -0.0823, -0.0238],\n",
            "        ...,\n",
            "        [ 0.0328, -0.0300, -0.0938,  ...,  0.0974,  0.0736, -0.0980],\n",
            "        [ 0.0929, -0.0437,  0.0066,  ..., -0.0608,  0.0305, -0.0839],\n",
            "        [ 0.0365,  0.0963,  0.0389,  ...,  0.0930,  0.0037,  0.0639]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.0716,  0.0075, -0.0487, -0.0811, -0.0541,  0.0498, -0.0461, -0.0887,\n",
            "        -0.0476, -0.0401, -0.0809, -0.0810,  0.0171,  0.0654, -0.0073, -0.0620,\n",
            "         0.0525, -0.0219,  0.0541, -0.0175, -0.0835,  0.0030,  0.0546,  0.0902,\n",
            "         0.0802, -0.0322, -0.0715, -0.0997,  0.0457,  0.0302, -0.0401, -0.0703,\n",
            "        -0.0804, -0.0621, -0.0111, -0.0980, -0.0082, -0.0166,  0.0260, -0.0600,\n",
            "         0.0065,  0.0659, -0.0091,  0.0411, -0.0919,  0.0228, -0.0674,  0.0454,\n",
            "         0.0522, -0.0842, -0.0339,  0.0640,  0.0590,  0.0051, -0.0110, -0.0814,\n",
            "         0.0658,  0.0428,  0.0129,  0.0650, -0.0952,  0.0953,  0.0326,  0.0715,\n",
            "         0.0784,  0.0514, -0.0025, -0.0105,  0.0070,  0.0719, -0.0435, -0.0475,\n",
            "         0.0180, -0.0147, -0.0965,  0.0112,  0.0761, -0.0951,  0.0449, -0.0381,\n",
            "        -0.0084,  0.0448,  0.0656, -0.0087,  0.0970, -0.0034, -0.0107,  0.0415,\n",
            "         0.0007,  0.0793, -0.0221, -0.0783,  0.0015, -0.0070,  0.0939, -0.0868,\n",
            "        -0.0914, -0.0938,  0.0032,  0.0084, -0.0382, -0.0707, -0.0342,  0.0252,\n",
            "         0.0331, -0.0251,  0.0350, -0.0267,  0.0921, -0.0676,  0.0044,  0.0280,\n",
            "        -0.0343, -0.0996,  0.0480,  0.0030,  0.1000, -0.0171,  0.0631,  0.0150,\n",
            "        -0.0236, -0.0241, -0.0931, -0.0463,  0.0921,  0.0231,  0.0448, -0.0658,\n",
            "        -0.0350, -0.0169,  0.0244,  0.0823,  0.0827,  0.0318, -0.0914,  0.0197,\n",
            "         0.0272,  0.0574, -0.0347, -0.0791, -0.0054,  0.0168,  0.0873, -0.0882,\n",
            "        -0.0317,  0.0929,  0.0385, -0.0815, -0.0235,  0.0377,  0.0302, -0.0652,\n",
            "        -0.0140,  0.0880, -0.0353,  0.0540,  0.0071,  0.0465, -0.0788,  0.0403,\n",
            "        -0.0646,  0.0737, -0.0177,  0.0636,  0.0516, -0.0720, -0.0313,  0.0356,\n",
            "        -0.0493,  0.0315, -0.0436, -0.0976,  0.0090,  0.0026,  0.0999, -0.0229,\n",
            "        -0.0153,  0.0854, -0.0856,  0.0909,  0.0582, -0.0611, -0.0546,  0.0422,\n",
            "         0.0624,  0.0450, -0.0984, -0.0514,  0.0898, -0.0315, -0.0797,  0.0532,\n",
            "         0.0869,  0.0414,  0.0899,  0.0982, -0.0189,  0.0069, -0.0726, -0.0821],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[ 0.0041,  0.0331,  0.0423,  ...,  0.0057,  0.0246,  0.0114],\n",
            "        [ 0.0488,  0.0534,  0.0647,  ...,  0.0306,  0.0397, -0.0456],\n",
            "        [-0.0341,  0.0409, -0.0572,  ..., -0.0031,  0.0656,  0.0304],\n",
            "        ...,\n",
            "        [ 0.0060, -0.0572,  0.0582,  ...,  0.0024, -0.0396, -0.0382],\n",
            "        [-0.0699,  0.0555,  0.0247,  ...,  0.0306,  0.0143,  0.0593],\n",
            "        [ 0.0080,  0.0690, -0.0594,  ..., -0.0049,  0.0211, -0.0317]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.0550, -0.0367, -0.0578, -0.0422,  0.0254,  0.0554, -0.0549,  0.0501,\n",
            "         0.0290,  0.0242], requires_grad=True)\n",
            "\n",
            "\n",
            "Layer params:\n",
            "Parameter containing:\n",
            "tensor([[ 0.0041,  0.0331,  0.0423,  ...,  0.0057,  0.0246,  0.0114],\n",
            "        [ 0.0488,  0.0534,  0.0647,  ...,  0.0306,  0.0397, -0.0456],\n",
            "        [-0.0341,  0.0409, -0.0572,  ..., -0.0031,  0.0656,  0.0304],\n",
            "        ...,\n",
            "        [ 0.0060, -0.0572,  0.0582,  ...,  0.0024, -0.0396, -0.0382],\n",
            "        [-0.0699,  0.0555,  0.0247,  ...,  0.0306,  0.0143,  0.0593],\n",
            "        [ 0.0080,  0.0690, -0.0594,  ..., -0.0049,  0.0211, -0.0317]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.0550, -0.0367, -0.0578, -0.0422,  0.0254,  0.0554, -0.0549,  0.0501,\n",
            "         0.0290,  0.0242], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lin = torch.nn.Linear(3, 2)\n",
        "x = torch.rand(1, 3)\n",
        "print('input: ')\n",
        "print(x)\n",
        "\n",
        "print('\\n\\nWeight and Bias parameters: ')\n",
        "for param in lin.parameters():\n",
        "  print(param)\n",
        "\n",
        "y = lin(x)\n",
        "print('\\n\\nOutput: ')\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_8fxaV6dtvy",
        "outputId": "94a53956-056f-4e8b-ac6c-fd89703f17c8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input: \n",
            "tensor([[0.3686, 0.0098, 0.1109]])\n",
            "\n",
            "\n",
            "Weight and Bias parameters: \n",
            "Parameter containing:\n",
            "tensor([[-0.0674,  0.1185,  0.3374],\n",
            "        [ 0.2655,  0.5513,  0.3130]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0755, -0.3344], requires_grad=True)\n",
            "\n",
            "\n",
            "Output: \n",
            "tensor([[-0.0617, -0.1964]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.functional as F\n",
        "\n",
        "class LeNet(torch.nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "      super(LeNet, self).__init__()\n",
        "      # 1 input image channel (black & white), \n",
        "      # 6 output channels, 5x5 square convolution\n",
        "      # kernel\n",
        "      self.conv1 = torch.nn.Conv2d(1, 6, 5)\n",
        "      self.conv2 = torch.nn.Conv2d(6, 16, 3)\n",
        "      self.fc1 = torch.nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n",
        "      self.fc2 = torch.nn.Linear(120, 84)\n",
        "      self.fc3 = torch.nn.Linear(84, 10)\n",
        "\n",
        "  def fprward(self, x):\n",
        "      # Max pooling over a (2, 2) window\n",
        "      x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
        "      # If the size is a square you can only specify a single number\n",
        "      x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "      x = x.view(-1, self.num_flat_features(x))\n",
        "      x = F.relu(self.fc1(x))\n",
        "      x = F.relu(self.fc2(x))\n",
        "      x = self.fc3(x)\n",
        "      return x\n",
        "\n",
        "  def num_flat_features(self, x):\n",
        "    size = x.size()[1:] # all dimensions except the batch dimension\n",
        "    num_features = 1\n",
        "    for s in size:\n",
        "      num_features *= s\n",
        "    return num_features\n",
        "    "
      ],
      "metadata": {
        "id": "GHgmNnBaexIV"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}