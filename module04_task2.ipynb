{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "module04_task2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMOSq5chd+gnzwo8Qq4xpqA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlenaAntipina/PytorchLearning/blob/firstTest/module04_task2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmRbW8XlsUV6",
        "outputId": "209db519-c02f-461b-f47d-a61426e8e859"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fc_weight_grad: tensor([[10., 20.],\n",
            "        [10., 20.],\n",
            "        [10., 20.]])\n",
            "our_weight_grad: tensor([[10., 20.],\n",
            "        [10., 20.],\n",
            "        [10., 20.]])\n",
            "fc_bias_grad: tensor([[1., 1., 1.]])\n",
            "out_bias_grad: tensor([[1., 1., 1.]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Сперва создадим тензор x:\n",
        "x = torch.tensor([[10., 20.]])\n",
        "\n",
        "# Оригинальный полносвязный слой с 2-мя входами и 3-мя нейронами (выходами):\n",
        "fc = torch.nn.Linear(2, 3)\n",
        "\n",
        "# Веса fc-слоя хранятся в fc.weight, а bias'ы соответственно в fc.bias\n",
        "# fc.weight и fc.bias по умолчанию инициализируются случайными числами\n",
        "\n",
        "# Давайте проставим свои значения в веса и bias'ы:\n",
        "w = torch.tensor([[11., 12.], [21., 22.], [31., 32]])\n",
        "fc.weight.data = w\n",
        "\n",
        "b = torch.tensor([[31., 32., 33.]])\n",
        "fc.bias.data = b\n",
        "\n",
        "# Получим выход fc-слоя:\n",
        "fc_out = fc(x)\n",
        "# Просуммируем выход fc-слоя, чтобы получить скаляр:\n",
        "fc_out_summed = fc_out.sum()\n",
        "\n",
        "# Посчитаем градиенты формулы fc_out_summed:\n",
        "fc_out_summed.backward()\n",
        "weight_grad = fc.weight.grad\n",
        "bias_grad = fc.bias.grad\n",
        "\n",
        "# Ok, теперь воспроизведем вычисления выше но без fc-слоя:\n",
        "# Проставим, что у \"w\" и \"b\" нужно вычислять градиенты (для fc-слоя это произошло автоматически):\n",
        "w.requires_grad_(True)\n",
        "b.requires_grad_(True)\n",
        "\n",
        "# Получим выход нашей формулы:\n",
        "our_formula = (torch.mm(x, torch.t(w)) + b).sum()\n",
        "\n",
        "# Сделайте backward для нашей формулы:\n",
        "our_formula.backward()\n",
        "\n",
        "# Проверка осуществляется автоматически, вызовом функций:\n",
        "print('fc_weight_grad:', weight_grad)\n",
        "print('our_weight_grad:', w.grad)\n",
        "print('fc_bias_grad:', bias_grad)\n",
        "print('out_bias_grad:', b.grad)\n",
        "# (раскомментируйте, если работаете над задачей локально)"
      ]
    }
  ]
}